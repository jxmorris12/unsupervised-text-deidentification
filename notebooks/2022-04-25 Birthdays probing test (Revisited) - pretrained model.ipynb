{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54dd8d98-0748-478b-8c82-6b77cfa44e53",
   "metadata": {},
   "source": [
    "# Birthdays probing test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7d252cb9-4ee5-42f2-98df-af6e72555090",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jxm3/research/deidentification/unsupervised-deidentification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "529344f2-556a-49e3-92e4-b5055c9b0469",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized DocumentProfileMatchingTransformer with learning_rate = 1e-06\n"
     ]
    }
   ],
   "source": [
    "from model import DocumentProfileMatchingTransformer\n",
    "\n",
    "import os\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "model = DocumentProfileMatchingTransformer(\n",
    "    document_model_name_or_path='roberta-base',\n",
    "    profile_model_name_or_path='distilbert-base-uncased',\n",
    "    num_workers=min(8, num_cpus),\n",
    "    train_batch_size=64,\n",
    "    eval_batch_size=64,\n",
    "    learning_rate=1e-6,\n",
    "    max_seq_length=256,\n",
    "    pretrained_profile_encoder=False,\n",
    "    word_dropout_ratio=0.0,\n",
    "    word_dropout_perc=0.0,\n",
    "    lr_scheduler_factor=0.5,\n",
    "    lr_scheduler_patience=3,\n",
    "    adversarial_mask_k_tokens=0,\n",
    "    train_without_names=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29132408-7f12-472e-896b-494b99fda702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing WikipediaDataModule with num_workers = 8 and mask token `<mask>`\n",
      "loading wiki_bio[1.2.0] split train[:100%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading wiki_bio[1.2.0] split val[:20%]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default\n",
      "Reusing dataset wiki_bio (/home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da)\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-58e5e96e220311ed.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-778e9a6d1b0dfab7.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-3c4e94260fbd4dd3.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-9e279afc7bfb46f2.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-7c5ac0e6f364c103.arrow\n",
      "Loading cached processed dataset at /home/jxm3/.cache/huggingface/datasets/wiki_bio/default/1.2.0/c05ce066e9026831cd7535968a311fc80f074b58868cfdffccbc811dff2ab6da/cache-02418e1d9ade71ab.arrow\n"
     ]
    }
   ],
   "source": [
    "from dataloader import WikipediaDataModule\n",
    "import os\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "dm = WikipediaDataModule(\n",
    "    mask_token=model.document_tokenizer.mask_token,\n",
    "    dataset_name='wiki_bio',\n",
    "    dataset_train_split='train[:100%]',\n",
    "    dataset_val_split='val[:20%]',\n",
    "    dataset_version='1.2.0',\n",
    "    num_workers=min(8, num_cpus),\n",
    "    train_batch_size=64,\n",
    "    eval_batch_size=64,\n",
    ")\n",
    "dm.setup(\"fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea5280-96de-48cd-9ccc-23d8c3f3237d",
   "metadata": {},
   "source": [
    "## Get the birthday data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "774c9b7c-aedb-4ea6-9635-4c9ddd5c0639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "d = datetime.datetime.strptime('17 january 1943', \"%d %B %Y\")\n",
    "d.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2c6da6f-9788-4a13-bb05-46383951d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "\n",
    "def process_dataset(_dataset) -> List[Tuple[int, int]]:\n",
    "    _processed_data = []\n",
    "    for idx, d in enumerate(tqdm(_dataset, 'processing birthdays')):\n",
    "        profile = d['profile']\n",
    "        date_str_matches = re.search(r\"birth_date \\| ([\\d]{1,4} [a-z]+ [\\d]{1,4})\", profile)\n",
    "        if date_str_matches:\n",
    "            date_str = date_str_matches.group(1)\n",
    "            # print(date_str)\n",
    "            # parse to datetime.datetime\n",
    "            try:\n",
    "                dt = datetime.datetime.strptime(date_str, \"%d %B %Y\")\n",
    "            except ValueError as e:\n",
    "                # print(e)\n",
    "                continue\n",
    "            day_class_num = (dt.month - 1) * 31 + (dt.day - 1)\n",
    "            _processed_data.append((idx, day_class_num))\n",
    "    return _processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f793a8c-de55-4c12-8dd2-c3e6fa1f9e60",
   "metadata": {},
   "source": [
    "## Create birthday data module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce4eaea4-daf0-4996-96c9-0826a537b835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3f7aef90cc04fa9bfde881fed0bffa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing birthdays:   0%|          | 0/14566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning import LightningDataModule\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "class BirthdayDataModule(LightningDataModule):\n",
    "    train_dataset: List[Tuple[int, int]]\n",
    "    val_dataset: List[Tuple[int, int]]\n",
    "    batch_size: int\n",
    "    def __init__(self, dm: WikipediaDataModule, batch_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.train_dataset = process_dataset(dm.train_dataset)\n",
    "        self.val_dataset = process_dataset(dm.val_dataset)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = min(4, num_cpus)\n",
    "\n",
    "    def setup(self, stage: str) -> None:\n",
    "        return\n",
    "\n",
    "    def train_dataloader(self) -> process_dataset(dm.val_dataset):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False # Only shuffle for train\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader:\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=False\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13aa79a4-cced-4f94-a39b-1534759fd5ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "551c43fa545b465f8ae4557d0b23580a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing birthdays:   0%|          | 0/582659 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1955b70f1be497daee5dd4d0beaf05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "processing birthdays:   0%|          | 0/14566 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "birthday_dm = BirthdayDataModule(dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2982e8b-786a-4e7e-a0c8-957ecbef6e29",
   "metadata": {},
   "source": [
    "## Create birthday model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1057c57f-21af-4795-953b-68ab3a829427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precomputing profile embeddings before first epoch...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2af185100d54a3a8752e80c363aaa63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "[1/2] Precomputing train embeddings - profile:   0%|          | 0/9105 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def precompute_embeddings(model: DocumentProfileMatchingTransformer, datamodule: WikipediaDataModule):\n",
    "    model.profile_model.cuda()\n",
    "    model.profile_model.eval()\n",
    "    print('Precomputing profile embeddings before first epoch...')\n",
    "    \n",
    "    model.train_profile_embeddings = np.zeros((len(datamodule.train_dataset), model.profile_embedding_dim))\n",
    "    for train_batch in tqdm(datamodule.train_dataloader(), desc=\"[1/2] Precomputing train embeddings - profile\", colour=\"cyan\", leave=False):\n",
    "        with torch.no_grad():\n",
    "            profile_embeddings = model.forward_profile_text(text=train_batch[\"profile\"])\n",
    "        model.train_profile_embeddings[train_batch[\"text_key_id\"]] = profile_embeddings.cpu()\n",
    "    model.train_profile_embeddings = torch.tensor(model.train_profile_embeddings, dtype=torch.float32)\n",
    "    \n",
    "    model.val_profile_embeddings = np.zeros((len(datamodule.val_dataset), model.profile_embedding_dim))\n",
    "    for val_batch in tqdm(datamodule.val_dataloader(), desc=\"[2/2] Precomputing val embeddings - profile\", colour=\"green\", leave=False):\n",
    "        with torch.no_grad():\n",
    "            profile_embeddings = model.forward_profile_text(text=val_batch[\"profile\"])\n",
    "        model.val_profile_embeddings[val_batch[\"text_key_id\"]] = profile_embeddings.cpu()\n",
    "    model.val_profile_embeddings = torch.tensor(model.val_profile_embeddings, dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    model.profile_model.train()\n",
    "\n",
    "precompute_embeddings(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07850e58-4195-4b64-bc1c-899d3df4820b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import torch\n",
    "import torchmetrics\n",
    "import transformers\n",
    "\n",
    "from pytorch_lightning import LightningModule\n",
    "from transformers import AdamW\n",
    "\n",
    "class BirthdayModel(LightningModule):\n",
    "    \"\"\"Probes the PROFILE for birthday info.\"\"\"\n",
    "    profile_embeddings: torch.Tensor\n",
    "    classifier: torch.nn.Module\n",
    "    learning_rate: float\n",
    "    \n",
    "    def __init__(self, model: DocumentProfileMatchingTransformer, learning_rate: float):\n",
    "        super().__init__()\n",
    "        # We can pre-calculate these embeddings bc\n",
    "        self.train_profile_embeddings = torch.tensor(model.train_profile_embeddings.cpu())\n",
    "        self.val_profile_embeddings = torch.tensor(model.val_profile_embeddings.cpu())\n",
    "        self.classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(model.profile_embedding_dim, 64),\n",
    "            torch.nn.Dropout(p=0.01),\n",
    "            # 12 * 31 possible outputs\n",
    "            torch.nn.Linear(64, 12*31),\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_accuracy = torchmetrics.Accuracy()\n",
    "        self.val_accuracy   = torchmetrics.Accuracy()\n",
    "        self.loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def training_step(self, batch: Tuple[int, int], batch_idx: int) -> torch.Tensor:\n",
    "        profile_idxs, birthday_idxs = batch\n",
    "        assert ((0 <= profile_idxs) & (birthday_idxs < len(self.train_profile_embeddings))).all()\n",
    "        assert ((0 <= birthday_idxs) & (birthday_idxs < 12*31)).all()\n",
    "        # print('profile_idxs, birthday_idxs =', profile_idxs, birthday_idxs)\n",
    "        clf_device = next(self.classifier.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            embedding = self.train_profile_embeddings[profile_idxs].to(clf_device)\n",
    "        birthday_logits = self.classifier(embedding)\n",
    "        # loss = torch.nn.functional.cross_entropy(\n",
    "        #     birthday_logits, birthday_idxs\n",
    "        # )\n",
    "        # if batch_idx == 0: breakpoint()\n",
    "        self.log('train_accuracy', self.train_accuracy(birthday_logits, birthday_idxs))\n",
    "        if batch_idx % 300 == 0: print('train accuracy:', self.train_accuracy(birthday_logits, birthday_idxs))\n",
    "        return self.loss_criterion(birthday_logits, birthday_idxs)\n",
    "    \n",
    "    def validation_step(self, batch: Dict[str, torch.Tensor], batch_idx: int):\n",
    "        profile_idxs, birthday_idxs = batch\n",
    "        assert ((0 <= profile_idxs) & (profile_idxs < len(self.val_profile_embeddings))).all()\n",
    "        assert ((0 <= birthday_idxs) & (birthday_idxs < 12*31)).all()\n",
    "        # print('profile_idxs, birthday_idxs =', profile_idxs, birthday_idxs)\n",
    "        clf_device = next(self.classifier.parameters()).device\n",
    "        with torch.no_grad():\n",
    "            embedding = self.val_profile_embeddings[profile_idxs].to(clf_device)\n",
    "        # print('emebdding.shape:', embedding.shape)\n",
    "        birthday_logits = self.classifier(embedding)\n",
    "        # print('birthday_logits.shape:', birthday_logits.shape)\n",
    "        loss = torch.nn.functional.cross_entropy(\n",
    "            birthday_logits, birthday_idxs\n",
    "        )\n",
    "        if batch_idx == 0: self.log('val_accuracy', self.val_accuracy(birthday_logits, birthday_idxs))\n",
    "        return self.loss_criterion(birthday_logits, birthday_idxs)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"Prepare optimizer and schedule (linear warmup and decay)\"\"\"\n",
    "        optimizer = AdamW(\n",
    "            list(self.classifier.parameters()), lr=self.learning_rate\n",
    "        )\n",
    "        return optimizer\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e8da2e-b0c4-417e-8a39-34e2b20fdbb4",
   "metadata": {},
   "source": [
    "## Train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ae16d1-a388-4352-8e46-ef86bc8232ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning import Trainer, seed_everything\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "num_validations_per_epoch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca3e84-33e7-4cde-a4ad-a12334cbcb8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "birthday_model = BirthdayModel(model, 1e-3)\n",
    "birthday_dm.batch_size = 512\n",
    "\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "trainer = Trainer(\n",
    "    default_root_dir=f\"saves/jup/birthday_probing\",\n",
    "    val_check_interval=1.0,\n",
    "    max_epochs=25,\n",
    "    log_every_n_steps=50,\n",
    "    gpus=torch.cuda.device_count(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad30bce-e533-47d8-9cd0-756bd000973d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(birthday_model, birthday_dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ee97b0-bae0-4da5-a3b8-37c7a907a2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab25fb93-cb9e-4e9a-a240-82bb354a5e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch = next(iter(birthday_dm.val_dataloader()))\n",
    "\n",
    "def do_validation_batch(batch, batch_idx):\n",
    "    profile_idxs, birthday_idxs = batch\n",
    "    clf_device = next(birthday_model.classifier.parameters()).device\n",
    "    embedding = birthday_model.val_profile_embeddings[profile_idxs].to(clf_device)\n",
    "    print('emebdding.shape:', embedding.shape)\n",
    "    birthday_logits = birthday_model.classifier(embedding)\n",
    "    print('birthday_logits.shape:', birthday_logits.shape)\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        birthday_logits, birthday_idxs\n",
    "    )\n",
    "    # self.log('val_accuracy', self.val_accuracy(birthday_logits, birthday_idxs))\n",
    "    print('loss:', loss)\n",
    "\n",
    "do_validation_batch(val_batch, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4109e40-b072-4cc9-990b-ed8ea0b7801a",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_batch # last element: idx 85, birthday 55"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39966a7e-07e2-4656-bbdc-c9e990927b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "55 % 31 # february 24th\n",
    "\n",
    "dm.val_dataset[85]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cab00193-3ebf-4e66-9a56-dcac1efffd25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0936,  0.3824,  0.6874,  0.7990, -0.5991])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birthday_model.val_profile_embeddings[85][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "fa29c237-a9d7-4b3a-9e15-46e4a164e9ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0936,  0.3824,  0.6874,  0.7990, -0.5991], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.forward_profile_text(text=[dm.val_dataset[85]['profile']])[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9add825b-edad-46ba-8f55-0c9a54715782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch = next(iter(birthday_dm.train_dataloader()))\n",
    "train_batch # 682, 78\n",
    "78 % 31 # 16 -> this is march 17th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "a190c107-567b-4994-a35f-1b3691200206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'nationalgoals | 12\\nfullname | jes√∫s candelas rodrigo\\nmanagerclubs | netherlands assistant -rrb- iran netherlands malta thailand -lrb- assistant -rrb- hong kong malaysia netherlands -lrb-\\nname | victor hermans\\narticle_title | victor hermans\\nnationalyears | 1977 -- 1989\\nposition | manager -lrb- association football -rrb-\\ncurrentclub | thailand national futsal team -lrb- head coach -rrb-\\nclubs | mvv maastricht k.s.k. tongeren\\nnationalteam | netherlands -lrb- futsal -rrb-\\nbirth_place | maastricht , netherlands\\nbirth_date | 17 march 1953\\nnationalcaps | 50\\nmanageryears | 1990 2000 2001 2001-2007 2009 -- 2011 2012 -- -- 1992 1992 -- 1996 1996 1997 --\\nheight | 1.72'"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm.train_dataset[682]['profile']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "9ed50693-d79b-41c3-8b4b-5e95a5b28822",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3974,  0.4090,  0.3919,  1.2626, -0.1960])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birthday_model.train_profile_embeddings[682][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b09f9a55-3d33-414d-83c1-743be75183fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3974,  0.4090,  0.3919,  1.2626, -0.1960], device='cuda:0',\n",
       "       grad_fn=<SliceBackward0>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "model.forward_profile_text(text=[dm.train_dataset[682]['profile']])[0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "54081f5a-d022-41b6-b342-d15ebc793b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('0.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0123,  0.0198, -0.0286,  ...,  0.0052, -0.0202,  0.0360],\n",
       "          [-0.0107,  0.0232,  0.0180,  ...,  0.0116, -0.0154, -0.0274],\n",
       "          [-0.0222, -0.0221,  0.0122,  ...,  0.0234,  0.0198,  0.0023],\n",
       "          ...,\n",
       "          [ 0.0127,  0.0177, -0.0266,  ..., -0.0159, -0.0071,  0.0111],\n",
       "          [-0.0245,  0.0075,  0.0298,  ..., -0.0179, -0.0173,  0.0030],\n",
       "          [ 0.0115,  0.0255,  0.0330,  ..., -0.0075, -0.0049, -0.0297]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('0.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0013,  0.0079,  0.0005, -0.0231,  0.0133, -0.0023,  0.0213, -0.0355,\n",
       "          -0.0328, -0.0144, -0.0042,  0.0066, -0.0263, -0.0157,  0.0100,  0.0275,\n",
       "           0.0136, -0.0305, -0.0026, -0.0168,  0.0358, -0.0242,  0.0104,  0.0301,\n",
       "           0.0180,  0.0171,  0.0291,  0.0126,  0.0347,  0.0225,  0.0016, -0.0308,\n",
       "           0.0349, -0.0179, -0.0320,  0.0195, -0.0254,  0.0104,  0.0150, -0.0162,\n",
       "           0.0283, -0.0039, -0.0328, -0.0060, -0.0165, -0.0120, -0.0170, -0.0235,\n",
       "          -0.0352,  0.0144, -0.0301, -0.0137,  0.0017,  0.0382,  0.0244, -0.0185,\n",
       "           0.0283, -0.0119,  0.0005, -0.0137,  0.0091,  0.0157, -0.0030,  0.0207],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('2.weight',\n",
       "  Parameter containing:\n",
       "  tensor([[-0.0337, -0.0920,  0.0875,  ..., -0.0636, -0.0773, -0.0574],\n",
       "          [-0.0012,  0.0175, -0.0539,  ...,  0.0705, -0.0700, -0.1099],\n",
       "          [ 0.0043,  0.0764,  0.1039,  ...,  0.0481, -0.0374,  0.0520],\n",
       "          ...,\n",
       "          [-0.0468,  0.0516, -0.0190,  ...,  0.0290,  0.0435,  0.0048],\n",
       "          [ 0.0811,  0.0976,  0.1089,  ...,  0.0472,  0.0555, -0.0006],\n",
       "          [ 0.1114,  0.0204, -0.0691,  ..., -0.0351,  0.0810,  0.0931]],\n",
       "         device='cuda:0', requires_grad=True)),\n",
       " ('2.bias',\n",
       "  Parameter containing:\n",
       "  tensor([-0.0870,  0.0729, -0.0289, -0.0677, -0.1012, -0.0503, -0.1068, -0.0736,\n",
       "          -0.0143, -0.0717,  0.1077,  0.1260,  0.0432,  0.0496,  0.0426,  0.0949,\n",
       "          -0.0696,  0.0289,  0.0497, -0.0227, -0.0836,  0.1024,  0.0082,  0.0697,\n",
       "           0.1041,  0.0171, -0.0586,  0.0934, -0.0383,  0.0688,  0.0113,  0.0479,\n",
       "          -0.0159,  0.0595,  0.0331,  0.0836,  0.0340,  0.0831, -0.0236,  0.0065,\n",
       "          -0.0666,  0.0432,  0.1012,  0.0874, -0.0021,  0.0061,  0.1215, -0.0815,\n",
       "          -0.1043, -0.1065,  0.0998, -0.0668, -0.0047, -0.0433,  0.1000,  0.0227,\n",
       "           0.0069, -0.0552, -0.0885, -0.0664, -0.0543, -0.0206,  0.0647,  0.0276,\n",
       "          -0.0650, -0.0591,  0.0337, -0.0533, -0.1014,  0.0148, -0.1074,  0.0308,\n",
       "          -0.0283, -0.0592, -0.0138,  0.0201, -0.0741,  0.1059,  0.0424, -0.0520,\n",
       "           0.0312, -0.0373,  0.1128, -0.0843, -0.0305,  0.0604,  0.1067, -0.0756,\n",
       "          -0.0319, -0.0095,  0.0547, -0.0938,  0.0381, -0.0927, -0.0105, -0.0383,\n",
       "          -0.0553,  0.0072, -0.0008, -0.0551,  0.0037, -0.0960,  0.0292,  0.0733,\n",
       "          -0.0770, -0.0393,  0.0506,  0.0515, -0.0379,  0.0058,  0.0905, -0.1010,\n",
       "           0.0301, -0.0783, -0.0690, -0.0564, -0.0969, -0.0117,  0.0309,  0.0514,\n",
       "           0.0855, -0.0640, -0.1065,  0.0596, -0.0990, -0.1059,  0.1231, -0.0824,\n",
       "          -0.1071,  0.0441,  0.0796, -0.1159,  0.1165,  0.0653,  0.0500,  0.0850,\n",
       "          -0.1092, -0.1192,  0.1046,  0.0314, -0.0910,  0.0083, -0.0866, -0.0979,\n",
       "          -0.0468, -0.0417, -0.0700,  0.1078, -0.0765,  0.1205, -0.1118, -0.0391,\n",
       "           0.0744,  0.0143,  0.0579, -0.0214, -0.0187,  0.0729, -0.1145,  0.0646,\n",
       "           0.0203, -0.0803,  0.0310,  0.0142,  0.0529, -0.1170, -0.0574, -0.0777,\n",
       "           0.0722,  0.1140,  0.0138,  0.0686,  0.0280,  0.0759,  0.0291,  0.0638,\n",
       "          -0.0835, -0.0931, -0.0023, -0.0523,  0.0399,  0.1182,  0.0241,  0.0227,\n",
       "           0.0461, -0.0870, -0.0151,  0.1126,  0.0321,  0.1166,  0.0236,  0.0940,\n",
       "           0.1077, -0.0153, -0.1018,  0.1101,  0.0903, -0.0384,  0.0784,  0.0968,\n",
       "          -0.0963,  0.1054, -0.0868,  0.0435, -0.0348,  0.1197, -0.0928,  0.0439,\n",
       "          -0.0535,  0.1018, -0.0820,  0.0969, -0.0435, -0.0491,  0.0295,  0.0219,\n",
       "          -0.1105, -0.0891,  0.0638,  0.0108,  0.0004, -0.0958,  0.0603,  0.0995,\n",
       "           0.0916, -0.0900, -0.0864,  0.0795, -0.0619, -0.1045,  0.1262, -0.0020,\n",
       "          -0.0095, -0.0310,  0.1080,  0.1055, -0.0381, -0.0963,  0.0451, -0.0762,\n",
       "           0.0488, -0.1098,  0.0326,  0.0781, -0.1216,  0.1130,  0.0296,  0.0486,\n",
       "           0.0601,  0.0250, -0.0084, -0.0991, -0.0696,  0.0300,  0.1073,  0.1068,\n",
       "          -0.0739, -0.0928,  0.0370,  0.0423,  0.0324,  0.0262, -0.1137, -0.0627,\n",
       "           0.1169,  0.0193,  0.0189, -0.0620, -0.0192,  0.1051,  0.0163,  0.0367,\n",
       "          -0.0548,  0.0488,  0.0557,  0.0543, -0.0147,  0.0607,  0.0415,  0.0175,\n",
       "           0.0685, -0.0815, -0.0398,  0.0738,  0.0428,  0.0353,  0.0300,  0.1131,\n",
       "          -0.1003,  0.0438, -0.0313,  0.1084,  0.0659, -0.0906,  0.0310,  0.0552,\n",
       "           0.0548, -0.0414, -0.0009,  0.0893, -0.0711,  0.0315, -0.0204, -0.0999,\n",
       "           0.1099,  0.0822, -0.0023, -0.0762,  0.0335,  0.0416,  0.0140, -0.0180,\n",
       "          -0.0040,  0.0926, -0.0071,  0.0358,  0.0071, -0.0459,  0.0902,  0.0263,\n",
       "          -0.0395, -0.0252, -0.0707,  0.0845, -0.0741, -0.0770,  0.0208,  0.1066,\n",
       "           0.0664, -0.0090,  0.0465,  0.0395, -0.0772,  0.0421,  0.0837,  0.0723,\n",
       "           0.0209,  0.0426,  0.0318, -0.0349,  0.0747, -0.0111, -0.0689,  0.0749,\n",
       "          -0.0526,  0.0575,  0.0819,  0.0539,  0.0136,  0.0303, -0.0580,  0.0291,\n",
       "          -0.0238,  0.0401,  0.0906,  0.0128, -0.0979, -0.0738, -0.0750, -0.0478,\n",
       "          -0.0342,  0.0235, -0.0975, -0.0043, -0.0867, -0.0494,  0.0917,  0.0062,\n",
       "           0.0014, -0.0199, -0.1200, -0.1088], device='cuda:0',\n",
       "         requires_grad=True))]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(birthday_model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3929e3d8-5928-4038-8fd1-b14457c4228a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
